# -*- coding: utf-8 -*-
"""ModelBuild.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pLGdaBteD5MA4O2IHIy0qSUzCsHqbLeV
"""

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('Roman Urdu DataSet.csv')
df.dropna()
df.columns =['comments', 'Result','NaN']

# Making all the comments lower case
df['comments'] = df['comments'].str.lower()
df.head()

#Deleting the third column
df.drop('NaN', inplace=True, axis=1)
df.head()

#Removing null values in comment
df['comments'].isnull().sum()
df = df.dropna(axis=0, inplace=False)

df['comments'].isnull().sum()

df['comments'].value_counts().sum()

df["Result"].unique() #checking unique values in the dataset

## Correcting the spelling of one misspelled label
df['Result'] = df['Result'].str.replace('Neative', 'Negative')

df["Result"].unique() #checking unique values in the dataset

#Counting all the values according to the sentiments
print("Positive --> "+ str(len(df[df['Result']=='Positive'])))
print("Negative --> "+ str(len(df[df['Result']=='Negative'])))
print("Neutral --> "+ str(len(df[df['Result']=='Neutral'])))

df.shape

#Bar graph for the classes
import matplotlib.pyplot as plt

Pos = df[df['Result'] == 'Positive'].shape[0]
Neg = df[df['Result'] == 'Negative'].shape[0]
Neu = df[df['Result'] == 'Neutral'].shape[0]

plt.bar(10,Pos,3, label="Positve")
plt.bar(15,Neg,3, label="Negative")
plt.bar(20,Neu,3, label="Neutral")
plt.legend()
plt.ylabel('Number of examples')
plt.title('Proportion of examples')
plt.show()

#punctuation
df['without_punctuation']=df['comments'].str.replace(r'[^\w\s]+', '')
df.head(5)

df.drop('comments', inplace=True, axis=1)
df.head()

#Changing column names 
df.rename(columns = {'without_punctuation':'comments', 'Result':'Sentiments'}, inplace = True)
df.head()

import nltk
import re as re
nltk.download('punkt')
from nltk.tokenize import word_tokenize
df['tokenized_comments'] = df.apply(lambda row: nltk.word_tokenize(row['comments']), axis=1)
df.head()

# Only keeping the tokeized_comments column with sentiments and removing comments
df.drop('comments', inplace=True, axis=1)
df.head()

# 2916 more instances are needed for making Positive equal to Neutral
# 3,641 more instance needed for making Negative  equal to Neutral

# Data Augmentation using random shuffling, one of the most common techniques 
8928
import random
# This function perform data augmentation using tokenized word swapping
def data_augmentation(dataset, percent):

  comments = []
  sentiments = []
  # How much percent data should be augmented 
  new_df = dataset.sample(frac=percent)
  #df.apply(lambda x: random.shuffle(x['tokenized_comments']), axis=1)
  copy_of_new_df = new_df
  for ind in new_df.index:
    comments.append(new_df['tokenized_comments'][ind])
    sentiments.append(new_df['Sentiments'][ind])
  
  for itr in range(len(comments)):
    random.shuffle(comments[itr])

  
  new_df = pd.DataFrame(list(zip(sentiments, comments)),
               columns =['Sentiments', 'tokenized_comments'])

  return new_df

df_pos = df[df['Sentiments'] == 'Positive']
df_neg = df[df['Sentiments'] == 'Negative']
df_pos.head(5)
df_pos.shape

pos_augmented_df = data_augmentation(df_pos, 0.4850)
neg_augmented_df = data_augmentation(df_neg, 0.6886)
pos_augmented_df.head(5)

pos_augmented_df.shape

neg_augmented_df.shape

df = pd.concat([df, pos_augmented_df, neg_augmented_df], axis=0)
df.shape

#Counting all the values according to the sentiments
print("Positive --> "+ str(len(df[df['Sentiments']=='Positive'])))
print("Negative --> "+ str(len(df[df['Sentiments']=='Negative'])))
print("Neutral --> "+ str(len(df[df['Sentiments']=='Neutral'])))

Pos = df[df['Sentiments'] == 'Positive'].shape[0]
Neg = df[df['Sentiments'] == 'Negative'].shape[0]
Neu = df[df['Sentiments'] == 'Neutral'].shape[0]

plt.bar(10,Pos,3, label="Positve")
plt.bar(15,Neg,3, label="Negative")
plt.bar(20,Neu,3, label="Neutral")
plt.legend()
plt.ylabel('Number of examples')
plt.title('Proportion of examples')
plt.show()

import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer

df['tokenized_comments'] = df['tokenized_comments'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))

X=df["tokenized_comments"]
y=df["Sentiments"]

df2 = pd.DataFrame().assign(tweet=df["tokenized_comments"], output=df["Sentiments"])

df2.head()

train, test = train_test_split(df2, test_size = 0.2, stratify = df2['output'], random_state=21)

# get the shape of train and test split.
train.shape, test.shape

from sklearn.feature_extraction.text import  TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split

stopwords=['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', 'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya', 'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se', 'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski', 'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya', 'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi', 'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain', 'krny', 'tou']

stop_extended =  stopwords

tfidf_vectorizer = TfidfVectorizer(lowercase= True, max_features=1000, stop_words=stop_extended)

# fit the object with the training data tweets
tfidf_vectorizer.fit(train.tweet)

train_idf = tfidf_vectorizer.transform(train.tweet)
test_idf  = tfidf_vectorizer.transform(test.tweet)

from sklearn.svm import SVC
clf=SVC()
clf.fit(train_idf, train.output)

predict_train1 = clf.predict(train_idf)

predict_test1 =  clf.predict(test_idf)

from nltk.metrics.scores import accuracy
print (accuracy(test.output,predict_test1))

pipeline = Pipeline(steps= [('tfidf', TfidfVectorizer(lowercase=True,
                                                      max_features=1000,
                                                      stop_words= stop_extended)),
                            ('model', LogisticRegression())])

# fit the pipeline model with the training data                            
pipeline.fit(train.tweet, train.output)

text = ["oppo nahi lena"]

# predict the label using the pipeline
pipeline.predict(text)

from joblib import dump

# dump the pipeline model
dump(pipeline, filename="text_classification.joblib")

import pandas as pd
from joblib import load
text = ["oppo bohot acha ha"]
pipeline = load("text_classification.joblib")
pipeline.predict(text)



